{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_copy import load_fabric_data, extract_label_grouping, extract_label_grouping, load_fabric_images\n",
    "import numpy as np\n",
    "from matplotlib import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r\"C:/Users/Administrator/Desktop/PRML/Project/fabric_data/label_json/**/**.json\"\n",
    "\n",
    "fids, fdata = load_fabric_data(path)\n",
    "ftype1, ftype2 = extract_label_grouping(fdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Administrator/Desktop/PRML/Project/fabric_data/temp/**/**.jpg\n",
      "3371\n",
      "[[[130 130 128 155 147 171]\n",
      "  [139 137 138 151 150 168]\n",
      "  [147 145 148 146 154 167]\n",
      "  ...\n",
      "  [ 57  46  24  92  87  29]\n",
      "  [ 73  51  28  83  64  34]\n",
      "  [124  89  69  76  47  51]]\n",
      "\n",
      " [[139 141 140 162 153 174]\n",
      "  [143 144 146 157 155 168]\n",
      "  [148 148 150 151 155 166]\n",
      "  ...\n",
      "  [ 69  61  38 121 124  69]\n",
      "  [ 58  41  15 113 100  56]\n",
      "  [ 69  39  15  88  63  33]]\n",
      "\n",
      " [[143 147 150 160 149 166]\n",
      "  [145 149 152 156 153 162]\n",
      "  [150 151 156 153 156 161]\n",
      "  ...\n",
      "  [102 100  75 153 160 116]\n",
      "  [ 90  79  49 140 133  79]\n",
      "  [ 75  55  22 126 106  47]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 56  19  13  76  21  24]\n",
      "  [ 55  16  11  77  28  31]\n",
      "  [ 57  18  13  70  31  32]\n",
      "  ...\n",
      "  [208 213 219 210 213 206]\n",
      "  [208 218 220 222 223 228]\n",
      "  [200 210 211 216 213 234]]\n",
      "\n",
      " [[ 74  35  28 103  31  34]\n",
      "  [ 69  30  23 103  37  39]\n",
      "  [ 72  31  25  95  39  40]\n",
      "  ...\n",
      "  [191 200 209 212 212 212]\n",
      "  [220 231 237 209 209 221]\n",
      "  [196 210 213 213 210 237]]\n",
      "\n",
      " [[ 87  49  40 122  36  37]\n",
      "  [ 82  42  34 123  41  43]\n",
      "  [ 85  45  37 115  43  46]\n",
      "  ...\n",
      "  [185 193 204 214 212 223]\n",
      "  [195 208 216 215 212 233]\n",
      "  [193 208 215 217 216 248]]]\n",
      "Number of samples: 3371\n",
      "(400, 400, 6)\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:/Users/Administrator/Desktop/PRML/Project/fabric_data/temp/\"\n",
    "labels, imgs = load_fabric_images(path, fids, fdata, ftype2)\n",
    "\n",
    "print(len(labels))\n",
    "print(imgs[0])\n",
    "\n",
    "n_samples = len(imgs)\n",
    "print(\"Number of samples:\", n_samples)\n",
    "\n",
    "print(imgs[1230].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "- https://blog.csdn.net/wuzqChom/article/details/74785643\n",
    "- https://stackoverflow.com/questions/47697622/cnn-image-resizing-vs-padding-keeping-aspect-ratio-or-not/49882055#49882055\n",
    "- https://stackoverflow.com/questions/43391205/add-padding-to-images-to-get-them-into-the-same-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [cv2.resize(img,(200, 200)) for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training data: 2359\n",
      "#Testing data: 1012\n",
      "#Class: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"#Training data: {}\\n#Testing data: {}\\n#Class: {}\".format(len(train_images), len(test_images), len(set(train_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = np.array(train_images), np.array(test_images), np.array(train_labels), np.array(test_labels)\n",
    "# train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 8 3 ... 7 4 4]\n"
     ]
    }
   ],
   "source": [
    "train_images.shape\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'freq': test_labels})\n",
    "df.groupby('freq', as_index=False).size().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model\n",
    "- https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 50, 50, 96)        69792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 50, 50, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 50, 50, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 25, 25, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 25, 25, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4096)              51384320  \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 9)                 9009      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 9)                 36        \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 76,095,997\n",
      "Trainable params: 76,074,843\n",
      "Non-trainable params: 21,154\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "#Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(200,200,6), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(200,200,6,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "#lexNet.add(Dropout(0.1))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "#lexNet.add(Dropout(0.1))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "#lexNet.add(Dropout(0.1))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(9))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "AlexNet.compile(optimizer= tf.keras.optimizers.Adam(0.0001),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 303s 4s/step - loss: 1.5429 - accuracy: 0.5146 - val_loss: 1.9666 - val_accuracy: 0.2530\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 305s 4s/step - loss: 1.1162 - accuracy: 0.7198 - val_loss: 1.6819 - val_accuracy: 0.4881\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 302s 4s/step - loss: 0.9338 - accuracy: 0.8131 - val_loss: 1.5266 - val_accuracy: 0.5375\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 301s 4s/step - loss: 0.8634 - accuracy: 0.8296 - val_loss: 1.4276 - val_accuracy: 0.6176\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 349s 5s/step - loss: 0.7801 - accuracy: 0.8788 - val_loss: 1.4020 - val_accuracy: 0.6354\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 348s 5s/step - loss: 0.7219 - accuracy: 0.9008 - val_loss: 1.4002 - val_accuracy: 0.6413\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 330s 4s/step - loss: 0.6834 - accuracy: 0.9173 - val_loss: 1.3308 - val_accuracy: 0.6749\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 311s 4s/step - loss: 0.6666 - accuracy: 0.9165 - val_loss: 1.3025 - val_accuracy: 0.6887\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 311s 4s/step - loss: 0.6254 - accuracy: 0.9305 - val_loss: 1.2887 - val_accuracy: 0.6798\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 311s 4s/step - loss: 0.5901 - accuracy: 0.9419 - val_loss: 1.3341 - val_accuracy: 0.6423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1884e5ba7f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "AlexNet.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reason why accuracy doesn't further increase\n",
    "1. imbalance of trianing set \n",
    "2. learning rate too large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 22s - loss: 1.3341 - accuracy: 0.6423\n"
     ]
    }
   ],
   "source": [
    "#plt.plot(history.history['accuracy'], label='accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([0.5, 1])\n",
    "#plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = AlexNet.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
