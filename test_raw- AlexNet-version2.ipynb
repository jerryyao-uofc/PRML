{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_copy import load_fabric_data, extract_label_grouping, extract_label_grouping, load_fabric_images\n",
    "import numpy as np\n",
    "from matplotlib import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r\"C:/Users/Administrator/Desktop/PRML/Project/fabric_data/label_json/**/**.json\"\n",
    "\n",
    "fids, fdata = load_fabric_data(path)\n",
    "ftype1, ftype2 = extract_label_grouping(fdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Administrator/Desktop/PRML/Project/fabric_data/temp/**/**.jpg\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:/Users/Administrator/Desktop/PRML/Project/fabric_data/temp/\"\n",
    "labels, imgs = load_fabric_images(path, fids, fdata, ftype2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371\n",
      "[[[130 130 128 155 147 171]\n",
      "  [139 137 138 151 150 168]\n",
      "  [147 145 148 146 154 167]\n",
      "  ...\n",
      "  [ 57  46  24  92  87  29]\n",
      "  [ 73  51  28  83  64  34]\n",
      "  [124  89  69  76  47  51]]\n",
      "\n",
      " [[139 141 140 162 153 174]\n",
      "  [143 144 146 157 155 168]\n",
      "  [148 148 150 151 155 166]\n",
      "  ...\n",
      "  [ 69  61  38 121 124  69]\n",
      "  [ 58  41  15 113 100  56]\n",
      "  [ 69  39  15  88  63  33]]\n",
      "\n",
      " [[143 147 150 160 149 166]\n",
      "  [145 149 152 156 153 162]\n",
      "  [150 151 156 153 156 161]\n",
      "  ...\n",
      "  [102 100  75 153 160 116]\n",
      "  [ 90  79  49 140 133  79]\n",
      "  [ 75  55  22 126 106  47]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 56  19  13  76  21  24]\n",
      "  [ 55  16  11  77  28  31]\n",
      "  [ 57  18  13  70  31  32]\n",
      "  ...\n",
      "  [208 213 219 210 213 206]\n",
      "  [208 218 220 222 223 228]\n",
      "  [200 210 211 216 213 234]]\n",
      "\n",
      " [[ 74  35  28 103  31  34]\n",
      "  [ 69  30  23 103  37  39]\n",
      "  [ 72  31  25  95  39  40]\n",
      "  ...\n",
      "  [191 200 209 212 212 212]\n",
      "  [220 231 237 209 209 221]\n",
      "  [196 210 213 213 210 237]]\n",
      "\n",
      " [[ 87  49  40 122  36  37]\n",
      "  [ 82  42  34 123  41  43]\n",
      "  [ 85  45  37 115  43  46]\n",
      "  ...\n",
      "  [185 193 204 214 212 223]\n",
      "  [195 208 216 215 212 233]\n",
      "  [193 208 215 217 216 248]]]\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3371\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(imgs)\n",
    "print(\"Number of samples:\", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400, 6)\n"
     ]
    }
   ],
   "source": [
    "print(imgs[1230].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "- https://blog.csdn.net/wuzqChom/article/details/74785643\n",
    "- https://stackoverflow.com/questions/47697622/cnn-image-resizing-vs-padding-keeping-aspect-ratio-or-not/49882055#49882055\n",
    "- https://stackoverflow.com/questions/43391205/add-padding-to-images-to-get-them-into-the-same-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [cv2.resize(img,(200, 200)) for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(imgs, labels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training data: 2359\n",
      "#Testing data: 1012\n",
      "#Class: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"#Training data: {}\\n#Testing data: {}\\n#Class: {}\".format(len(train_images), len(test_images), len(set(train_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = np.array(train_images), np.array(test_images), np.array(train_labels), np.array(test_labels)\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 8 3 ... 7 4 4]\n"
     ]
    }
   ],
   "source": [
    "train_images.shape\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-89429501305b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'freq'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'freq'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'freq': test_labels})\n",
    "df.groupby('freq', as_index=False).size().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model\n",
    "- https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet model\n",
    "class AlexNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(96, kernel_size=(11,11), strides= 4,\n",
    "                        padding= 'valid', activation= 'relu',\n",
    "                        input_shape= input_shape,\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'valid', data_format= None))\n",
    "\n",
    "        self.add(Conv2D(256, kernel_size=(5,5), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'valid', data_format= None)) \n",
    "\n",
    "        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(Conv2D(256, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'valid', data_format= None))\n",
    "\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(4096, activation= 'relu'))\n",
    "        self.add(Dropout(0.4))\n",
    "        self.add(Dense(4096, activation= 'relu'))\n",
    "        self.add(Dropout(0.4))\n",
    "        self.add(Dense(1000, activation= 'relu'))\n",
    "        self.add(Dropout(0.4))\n",
    "        self.add(Dense(num_classes, activation= 'softmax'))\n",
    "\n",
    "        self.compile(optimizer= tf.keras.optimizers.Adam(0.0001),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet((200, 200, 6), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alex_net_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 48, 48, 96)        69792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 23, 23, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 23, 23, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 9009      \n",
      "=================================================================\n",
      "Total params: 50,887,865\n",
      "Trainable params: 50,887,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some training parameters\n",
    "EPOCHS = 10\n",
    "image_height = 200\n",
    "image_width = 200\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"validation\"\n",
    "model_dir = \"my_model.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 223s 3s/step - loss: 1.9808 - accuracy: 0.2556 - val_loss: 1.7445 - val_accuracy: 0.2846\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[6400,4096] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradient_tape/alex_net_1/dense_4/MatMul_1 (defined at <ipython-input-25-0eda1c78e52e>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1676]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0eda1c78e52e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model.fit(train_images, train_labels, epochs=10, \n\u001b[0m\u001b[0;32m      3\u001b[0m                     validation_data=(test_images, test_labels))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[6400,4096] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradient_tape/alex_net_1/dense_4/MatMul_1 (defined at <ipython-input-25-0eda1c78e52e>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1676]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reason why accuracy doesn't further increase\n",
    "1. imbalance of trianing set \n",
    "2. learning rate too large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history.history['accuracy'], label='accuracy')\n",
    "#plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([0.5, 1])\n",
    "#plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
